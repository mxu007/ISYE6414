plot(parameter_vals[[1]][[var_index]],col='black', ylab=vars[var_index])
points(points(parameter_vals[[2]][[var_index]],col='blue'))
points(points(parameter_vals[[3]][[var_index]],col='red'))
points(points(parameter_vals[[4]][[var_index]],col='green'))
legend("bottomright",legend=c('0.4', '0.8', '0.95', '1.0'), pch=1, col=c("black","blue","red","green"))
}
mtext('Stability of coefficient values vs Sample sizes', side = 3, line = -2, outer = TRUE)
#Plotting last variable
n_model_vars = 1
par(mfrow = c(3,3))
for (var_index in seq(19, 19+n_model_vars-1)){
plot(parameter_vals[[1]][[var_index]],col='black', ylab=vars[var_index])
points(points(parameter_vals[[2]][[var_index]],col='blue'))
points(points(parameter_vals[[3]][[var_index]],col='red'))
points(points(parameter_vals[[4]][[var_index]],col='green'))
legend("bottomright",legend=c('0.4', '0.8', '0.95', '1.0'), pch=1, col=c("black","blue","red","green"))
}
mtext('Stability of coefficient values vs Sample sizes', side = 3, line = -2, outer = TRUE)
#Plotting first 9 variables
n_model_vars = 9
par(mfrow = c(3,3))
for (var_index in seq(n_model_vars)){
plot(parameter_vals[[1]][[var_index]],col='black', ylab=vars[var_index])
points(points(parameter_vals[[2]][[var_index]],col='blue'))
points(points(parameter_vals[[3]][[var_index]],col='red'))
points(points(parameter_vals[[4]][[var_index]],col='green'))
legend("bottomright",legend=c('0.4', '0.8', '0.95', '1.0'), pch=1, col=c("black","blue","red","green"))
}
mtext('Stability of coefficient values vs Sample sizes', side = 3, line = -2, outer = TRUE)
#Plotting 10-18 variables
n_model_vars = 9
par(mfrow = c(3,3))
for (var_index in seq(10, 10+n_model_vars-1)){
plot(parameter_vals[[1]][[var_index]],col='black', ylab=vars[var_index])
points(points(parameter_vals[[2]][[var_index]],col='blue'))
points(points(parameter_vals[[3]][[var_index]],col='red'))
points(points(parameter_vals[[4]][[var_index]],col='green'))
legend("bottomright",legend=c('0.4', '0.8', '0.95', '1.0'), pch=1, col=c("black","blue","red","green"))
}
mtext('Stability of coefficient values vs Sample sizes', side = 3, line = -2, outer = TRUE)
#Plotting last variable
n_model_vars = 1
par(mfrow = c(3,3))
for (var_index in seq(19, 19+n_model_vars-1)){
plot(parameter_vals[[1]][[var_index]],col='black', ylab=vars[var_index])
points(points(parameter_vals[[2]][[var_index]],col='blue'))
points(points(parameter_vals[[3]][[var_index]],col='red'))
points(points(parameter_vals[[4]][[var_index]],col='green'))
legend("bottomright",legend=c('0.4', '0.8', '0.95', '1.0'), pch=1, col=c("black","blue","red","green"))
}
mtext('Stability of coefficient values vs Sample sizes', side = 3, line = -2, outer = TRUE)
# Define Accuracy Metrics
# Initiate Mean Square Error
mse_full<- mse_step_forward <- mse_step_backward <- mse_step_both <- mse_lasso <- mse_ridge <- mse_elastic <- NULL
#set.seed(999)
# Number of iterations for running the data
num_iter = 20
for (cnt in seq(num_iter)){
newdata_train_ind <- sample(1:nrow(newdata), floor(0.8*nrow(newdata)))
newdata_train <- newdata[newdata_train_ind,]
newdata_test <- newdata[-newdata_train_ind,]
dim(newdata_train)
dim(newdata_test)
X_test <- newdata_test[,-16]
y_test <- newdata_test[,16]
# Prediction on Test Set
# Full Model
full_model <- lm(log(utime)~.,data=newdata_train)
y_predict_full <- predict(full_model, X_test)
y_predict_full <- exp(y_predict_full)
mse_full <- cbind(mse_full,mean((y_predict_full-y_test)^2))
# Stepwise Regression Forward
min_model <- lm(log(utime)~umem, data = newdata_train)
step_forward_model <- step(min_model, scope=list(lower=min_model,upper=full_model), direction='forward')
y_predict_step_forward <- predict(step_forward_model, X_test)
y_predict_step_forward <- exp(y_predict_step_forward)
mse_step_forward <- cbind(mse_step_forward,mean((y_predict_step_forward-y_test)^2))
# Stepwise Regression Backward
step_back_model <- step(full_model, scope=list(lower=min_model,upper=full_model), direction='backward')
y_predict_step_backward <- predict(step_back_model, X_test)
y_predict_step_backward <- exp(y_predict_step_backward)
mse_step_backward <- cbind(mse_step_backward,mean((y_predict_step_backward-y_test)^2))
# Stepwise Regression Both
step_both_model <- stepAIC(full_model)
y_predict_step_both <- predict(step_both_model, X_test)
y_predict_step_both <- exp(y_predict_step_both)
mse_step_both <- cbind(mse_step_both,mean((y_predict_step_both-y_test)^2))
# Convert to matrix form for glmnet
X_train <- model.matrix(full_model)[,-1]
X_test <- (model.matrix(log(utime)~.,data=newdata_test))[,-1]
y_train <- log(newdata_train[,c('utime')])
# Lasso Regression
lasso_model_cv <- cv.glmnet(X_train, y_train, alpha = 1, nfolds=10)
lasso_model <- glmnet(X_train, y_train, alpha = lasso_model_cv$lambda.min)
#coef(lasso_model, s=lasso_model_cv$lambda.min)
y_predict_lasso <- predict(lasso_model, newx = X_test, s= lasso_model_cv$lambda.min, type="response")
y_predict_lasso <- exp(y_predict_lasso)
mse_lasso <- cbind(mse_lasso, mean((y_predict_lasso-y_test)^2))
# Ridge Regression
ridge_model_cv <- cv.glmnet(X_train, y_train, alpha = 0, nfolds=10)
ridge_model <- glmnet(X_train, y_train, alpha = ridge_model_cv$lambda.min)
y_predict_ridge <- predict(ridge_model, newx = X_test, s= ridge_model_cv$lambda.min, type="response")
y_predict_ridge <- exp(y_predict_ridge)
mse_ridge <- cbind(mse_ridge, mean((y_predict_ridge-y_test)^2))
# Elastic Net Regression
elastic_model_cv <- cv.glmnet(X_train, y_train, alpha = 0.5, nfolds=10)
elastic_model <- glmnet(X_train, y_train, alpha = elastic_model_cv$lambda.min)
y_predict_elastic <- predict(elastic_model, newx = X_test, s= elastic_model_cv$lambda.min, type="response")
y_predict_elastic <- exp(y_predict_elastic)
mse_elastic <- cbind(mse_elastic, mean((y_predict_elastic-y_test)^2))
}
model_names <- c("Full Model", "Stepwise Forward", "Step Backward", "Step Both", "Lasso", "Ridge", "Elastic Net")
list_mse <- list(mse_full,mse_step_forward,mse_step_backward,mse_step_both,mse_lasso,mse_ridge,mse_elastic)
par(mfrow = c(2,4))
for (i in seq(length(model_names))) {
plot(seq(num_iter), unlist(list_mse[i]), main = model_names[i], xlab="Iteration", ylab="MSE")
abline(h=mean(unlist(list_mse[i])), col="red")
#print("MSE of %s",model_names[i])
cat("MSE Summary of", model_names[i], "After",num_iter, "Runs :\n")
print(summary(unlist(list_mse[i])))
}
View(mse_full)
View(mse_step_backward)
View(mse_step_both)
View(mse_step_forward)
library(glmnet)
mse_full<- mse_step_forward <- mse_step_backward <- mse_step_both <- mse_lasso <- mse_ridge <- mse_elastic <- NULL
#set.seed(999)
# Number of iterations for running the data
num_iter = 20
for (cnt in seq(num_iter)){
newdata_train_ind <- sample(1:nrow(newdata), floor(0.8*nrow(newdata)))
newdata_train <- newdata[newdata_train_ind,]
newdata_test <- newdata[-newdata_train_ind,]
dim(newdata_train)
dim(newdata_test)
X_test <- newdata_test[,-16]
y_test <- newdata_test[,16]
# Prediction on Test Set
# Full Model
full_model <- lm(log(utime)~.,data=newdata_train)
y_predict_full <- predict(full_model, X_test)
y_predict_full <- exp(y_predict_full)
mse_full <- cbind(mse_full,mean((y_predict_full-y_test)^2))
# Stepwise Regression Forward
min_model <- lm(log(utime)~umem, data = newdata_train)
step_forward_model <- step(min_model, scope=list(lower=min_model,upper=full_model), direction='forward')
y_predict_step_forward <- predict(step_forward_model, X_test)
y_predict_step_forward <- exp(y_predict_step_forward)
mse_step_forward <- cbind(mse_step_forward,mean((y_predict_step_forward-y_test)^2))
# Stepwise Regression Backward
step_back_model <- step(full_model, scope=list(lower=min_model,upper=full_model), direction='backward')
y_predict_step_backward <- predict(step_back_model, X_test)
y_predict_step_backward <- exp(y_predict_step_backward)
mse_step_backward <- cbind(mse_step_backward,mean((y_predict_step_backward-y_test)^2))
# Stepwise Regression Both
step_both_model <- stepAIC(full_model)
y_predict_step_both <- predict(step_both_model, X_test)
y_predict_step_both <- exp(y_predict_step_both)
mse_step_both <- cbind(mse_step_both,mean((y_predict_step_both-y_test)^2))
# Convert to matrix form for glmnet
X_train <- model.matrix(full_model)[,-1]
X_test <- (model.matrix(log(utime)~.,data=newdata_test))[,-1]
y_train <- log(newdata_train[,c('utime')])
# Lasso Regression
lasso_model_cv <- cv.glmnet(X_train, y_train, alpha = 1, nfolds=10)
lasso_model <- glmnet(X_train, y_train, alpha = lasso_model_cv$lambda.min)
#coef(lasso_model, s=lasso_model_cv$lambda.min)
y_predict_lasso <- predict(lasso_model, newx = X_test, s= lasso_model_cv$lambda.min, type="response")
y_predict_lasso <- exp(y_predict_lasso)
mse_lasso <- cbind(mse_lasso, mean((y_predict_lasso-y_test)^2))
# Ridge Regression
ridge_model_cv <- cv.glmnet(X_train, y_train, alpha = 0, nfolds=10)
ridge_model <- glmnet(X_train, y_train, alpha = ridge_model_cv$lambda.min)
y_predict_ridge <- predict(ridge_model, newx = X_test, s= ridge_model_cv$lambda.min, type="response")
y_predict_ridge <- exp(y_predict_ridge)
mse_ridge <- cbind(mse_ridge, mean((y_predict_ridge-y_test)^2))
# Elastic Net Regression
elastic_model_cv <- cv.glmnet(X_train, y_train, alpha = 0.5, nfolds=10)
elastic_model <- glmnet(X_train, y_train, alpha = elastic_model_cv$lambda.min)
y_predict_elastic <- predict(elastic_model, newx = X_test, s= elastic_model_cv$lambda.min, type="response")
y_predict_elastic <- exp(y_predict_elastic)
mse_elastic <- cbind(mse_elastic, mean((y_predict_elastic-y_test)^2))
}
model_names <- c("Full Model", "Stepwise Forward", "Step Backward", "Step Both", "Lasso", "Ridge", "Elastic Net")
list_mse <- list(mse_full,mse_step_forward,mse_step_backward,mse_step_both,mse_lasso,mse_ridge,mse_elastic)
par(mfrow = c(2,4))
for (i in seq(length(model_names))) {
plot(seq(num_iter), unlist(list_mse[i]), main = model_names[i], xlab="Iteration", ylab="MSE")
abline(h=mean(unlist(list_mse[i])), col="red")
#print("MSE of %s",model_names[i])
cat("MSE Summary of", model_names[i], "After",num_iter, "Runs :\n")
print(summary(unlist(list_mse[i])))
}
mse_full<- mse_step_forward <- mse_step_backward <- mse_step_both <- mse_lasso <- mse_ridge <- mse_elastic <- NULL
set.seed(999)
# Number of iterations for running the data
k_fold = 10
newdata_cross <- newdata[sample(nrow(newdata)),]
folds <-cut(seq(1,nrow(newdata_cross)), breaks=k_fold, labels=FALSE)
for (cnt in seq(k_fold)){
newdata_test_ind <- which(folds==cnt, arr.ind = TRUE)
newdata_test <- newdata_cross[newdata_test_ind,]
newdata_train <- newdata_cross[-newdata_test_ind,]
head(newdata_train)
dim(newdata_train)
dim(newdata_test)
X_test <- newdata_test[,-16]
y_test <- newdata_test[,16]
# Prediction on Test Set
# Full Model
full_model <- lm(log(utime)~.,data=newdata_train)
y_predict_full <- predict(full_model, X_test)
y_predict_full <- exp(y_predict_full)
mse_full <- cbind(mse_full,mean((y_predict_full-y_test)^2))
# Stepwise Regression Forward
min_model <- lm(log(utime)~umem, data = newdata_train)
step_forward_model <- step(min_model, scope=list(lower=min_model,upper=full_model), direction='forward')
y_predict_step_forward <- predict(step_forward_model, X_test)
y_predict_step_forward <- exp(y_predict_step_forward)
mse_step_forward <- cbind(mse_step_forward,mean((y_predict_step_forward-y_test)^2))
# Stepwise Regression Backward
step_back_model <- step(full_model, scope=list(lower=min_model,upper=full_model), direction='backward')
y_predict_step_backward <- predict(step_back_model, X_test)
y_predict_step_backward <- exp(y_predict_step_backward)
mse_step_backward <- cbind(mse_step_backward,mean((y_predict_step_backward-y_test)^2))
# Stepwise Regression Both
step_both_model <- stepAIC(full_model)
y_predict_step_both <- predict(step_both_model, X_test)
y_predict_step_both <- exp(y_predict_step_both)
mse_step_both <- cbind(mse_step_both,mean((y_predict_step_both-y_test)^2))
# Convert to matrix form for glmnet
X_train <- model.matrix(full_model)[,-1]
X_test <- (model.matrix(log(utime)~.,data=newdata_test))[,-1]
y_train <- log(newdata_train[,c('utime')])
# Lasso Regression
lasso_model_cv <- cv.glmnet(X_train, y_train, alpha = 1, nfolds=10)
lasso_model <- glmnet(X_train, y_train, alpha = lasso_model_cv$lambda.min)
#coef(lasso_model, s=lasso_model_cv$lambda.min)
y_predict_lasso <- predict(lasso_model, newx = X_test, s= lasso_model_cv$lambda.min, type="response")
y_predict_lasso <- exp(y_predict_lasso)
mse_lasso <- cbind(mse_lasso, mean((y_predict_lasso-y_test)^2))
# Ridge Regression
ridge_model_cv <- cv.glmnet(X_train, y_train, alpha = 0, nfolds=10)
ridge_model <- glmnet(X_train, y_train, alpha = ridge_model_cv$lambda.min)
y_predict_ridge <- predict(ridge_model, newx = X_test, s= ridge_model_cv$lambda.min, type="response")
y_predict_ridge <- exp(y_predict_ridge)
mse_ridge <- cbind(mse_ridge, mean((y_predict_ridge-y_test)^2))
# Elastic Net Regression
elastic_model_cv <- cv.glmnet(X_train, y_train, alpha = 0.5, nfolds=10)
elastic_model <- glmnet(X_train, y_train, alpha = elastic_model_cv$lambda.min)
y_predict_elastic <- predict(elastic_model, newx = X_test, s= elastic_model_cv$lambda.min, type="response")
y_predict_elastic <- exp(y_predict_elastic)
mse_elastic <- cbind(mse_elastic, mean((y_predict_elastic-y_test)^2))
}
model_names <- c("Full Model", "Stepwise Forward", "Step Backward", "Step Both", "Lasso", "Ridge", "Elastic Net")
list_mse <- list(mse_full,mse_step_forward,mse_step_backward,mse_step_both,mse_lasso,mse_ridge,mse_elastic)
par(mfrow = c(2,4))
for (i in seq(length(model_names))) {
plot(seq(k_fold), unlist(list_mse[i]), main = model_names[i], xlab="Iteration", ylab="MSE")
abline(h=mean(unlist(list_mse[i])), col="red")
#print("MSE of %s",model_names[i])
cat("MSE Summary of", model_names[i], "With",k_fold, "Fold Validation :\n")
print(summary(unlist(list_mse[i])))
}
dim
dim(newdata)
dim(newdata_final_test)
61902+6879
dim(data)
data <- read.table("transcoding_measurement.tsv", sep='\t', header=TRUE)
data <- data[,-1]
set.seed(999)
data_ind <- sample(1:nrow(data), floor(0.9*nrow(data)))
data<- data[newdata_ind,]
data<- data[data_ind,]
dim(data)
data_ind
data <- read.table("transcoding_measurement.tsv", sep='\t', header=TRUE)
data <- data[,-1]
set.seed(999)
data_ind <- sample(1:nrow(data), floor(0.9*nrow(data)))
newdata_final_test <- (data[-newdata_ind,])
data_ind <- sample(1:nrow(data), floor(0.9*nrow(data)))
newdata_final_test <- (data[-data_ind,])
dim(newdata_final_test)
data<- data[data_ind,]
dim(data)
61905+6879
#Removing the frames column as i + p + b = frame
data <- data[ ,-which(names(data) == "frames")]
#Removing the size column as i_size + b_size + p_size = size
data <- data[ ,-which(names(data) == "size")]
#Removing the b_size column as its empty
data <- data[ ,-which(names(data) == "b_size")]
# Subset data by either factor or numeric
data_num <- data[,sapply(data,is.numeric)]
# Understanding correlations between variables
round(cor(data_num),2)
# Correlation Matrix
library(corrplot)
par(mfrow=c(1,1))
X = model.matrix(lm(utime ~ ., data = data))[,-1]
X = cbind(data$utime, X)
corrplot(cor(X), tl.cex = 1)
#Strong correlation seen in height and width variables
plot(data$height,data$width)
plot(data$o_height,data$o_width)
#Removing the width column as its highly correlated with height
data <- data[ ,-which(names(data) == "width")]
#Removing the width column as its highly correlated with height
data <- data[ ,-which(names(data) == "o_width")]
# Subset data by either factor or numeric
data_fac <- data[,sapply(data,is.factor)]
data_num <- data[,sapply(data,is.numeric)]
colnames(data_fac)
colnames(data_num)
# Frequency/Histogram Plot
par(mfrow=c(4,4))
for (col in colnames(data_fac)) {
plot(data_fac[,col], xlab=col, ylab="Frequency/Count", main=paste("Frequency of" , col)  )
}
for ( col in colnames(data_num)) {
hist(data[,col], xlab=col, ylab="Frequency/Count", main=paste("Histogram of" , col))
}
data_res <- data_num[,14]
data_num <- data_num[,-14]
par(mfrow=c(4,4))
for (col in colnames(data_fac)) {
boxplot(data_res~data_fac[,col], xlab=col, ylab="Transcoding Time in sec")
}
for (col in colnames(data_num)) {
plot(data_res~data_num[,col], xlab=col, ylab="Transcoding Time in sec")
}
attach(data)
#Fitting the model
model <- lm(log(utime)~.,data=data)
summary(model)
#Extracting specific parameter values
coef(summary(model))["duration","Pr(>|t|)"]
coef(summary(model))["duration","Estimate"]
#MLR Assumption validation
res = model$res
par(mfrow = c(4,4))
#Verifying Constant variance and Linearity
plot(utime, res, xlab = "utime", ylab = "Residuals", pch = 19, main = 'Residual vs utime')
abline(h = 0, col="red")
plot(height, res, xlab = "height", ylab = "Residuals", pch = 19, main = 'Residual vs height')
abline(h = 0,col="red")
plot(bitrate, res, xlab = "bitrate", ylab = "Residuals", pch = 19, main = 'Residual vs bitrate')
abline(h = 0,col="red")
plot(framerate, res, xlab = "framerate", ylab = "Residuals", pch = 19, main = 'Residual vs framerate')
abline(h = 0,col="red")
plot(i, res, xlab = "i", ylab = "Residuals", pch = 19, main = 'Residual vs i')
abline(h = 0,col="red")
plot(p, res, xlab = "p", ylab = "Residuals", pch = 19, main = 'Residual vs p')
abline(h = 0,col="red")
plot(b, res, xlab = "b", ylab = "Residuals", pch = 19, main = 'Residual vs b')
abline(h = 0,col="red")
plot(i_size, res, xlab = "i_size", ylab = "Residuals", pch = 19, main = 'Residual vs i_size')
abline(h = 0,col="red")
plot(p_size, res, xlab = "p_size", ylab = "Residuals", pch = 19, main = 'Residual vs p_size')
abline(h = 0,col="red")
plot(o_height, res, xlab = "o_height", ylab = "Residuals", pch = 19, main = 'Residual vs o_height')
abline(h = 0,col="red")
plot(o_bitrate, res, xlab = "o_bitrate", ylab = "Residuals", pch = 19, main = 'Residual vs o_bitrate')
abline(h = 0,col="red")
plot(o_framerate, res, xlab = "o_framerate", ylab = "Residuals", pch = 19, main = 'Residual vs o_framerate')
abline(h = 0,col="red")
plot(umem, res, xlab = "umem", ylab = "Residuals", pch = 19, main = 'Residual vs umem')
abline(h = 0,col="red")
#Verifying Normality of residuals
hist(res, xlab="Residuals", main= "Histogram of Residuals")
qqPlot(residuals(model),lwd=1, main = 'Normality of residuals')
#Identifying influencing datapoints
cookdist <- cooks.distance(model)
plot(cookdist, main = "Cook's Distance")
detach(data)
model <- lm(log(utime)~.,data=data)
summary(model)
#Extracting specific parameter values
coef(summary(model))["duration","Pr(>|t|)"]
coef(summary(model))["duration","Estimate"]
#MLR Assumption validation
res = model$res
par(mfrow = c(4,4))
#Verifying Constant variance and Linearity
plot(utime, res, xlab = "utime", ylab = "Residuals", pch = 19, main = 'Residual vs utime')
abline(h = 0, col="red")
plot(height, res, xlab = "height", ylab = "Residuals", pch = 19, main = 'Residual vs height')
abline(h = 0,col="red")
plot(bitrate, res, xlab = "bitrate", ylab = "Residuals", pch = 19, main = 'Residual vs bitrate')
abline(h = 0,col="red")
plot(framerate, res, xlab = "framerate", ylab = "Residuals", pch = 19, main = 'Residual vs framerate')
abline(h = 0,col="red")
plot(i, res, xlab = "i", ylab = "Residuals", pch = 19, main = 'Residual vs i')
abline(h = 0,col="red")
plot(p, res, xlab = "p", ylab = "Residuals", pch = 19, main = 'Residual vs p')
abline(h = 0,col="red")
plot(b, res, xlab = "b", ylab = "Residuals", pch = 19, main = 'Residual vs b')
abline(h = 0,col="red")
plot(i_size, res, xlab = "i_size", ylab = "Residuals", pch = 19, main = 'Residual vs i_size')
abline(h = 0,col="red")
plot(p_size, res, xlab = "p_size", ylab = "Residuals", pch = 19, main = 'Residual vs p_size')
abline(h = 0,col="red")
plot(o_height, res, xlab = "o_height", ylab = "Residuals", pch = 19, main = 'Residual vs o_height')
abline(h = 0,col="red")
plot(o_bitrate, res, xlab = "o_bitrate", ylab = "Residuals", pch = 19, main = 'Residual vs o_bitrate')
abline(h = 0,col="red")
plot(o_framerate, res, xlab = "o_framerate", ylab = "Residuals", pch = 19, main = 'Residual vs o_framerate')
abline(h = 0,col="red")
plot(umem, res, xlab = "umem", ylab = "Residuals", pch = 19, main = 'Residual vs umem')
abline(h = 0,col="red")
#Verifying Normality of residuals
hist(res, xlab="Residuals", main= "Histogram of Residuals")
qqPlot(residuals(model),lwd=1, main = 'Normality of residuals')
#Identifying influencing datapoints
cookdist <- cooks.distance(model)
plot(cookdist, main = "Cook's Distance")
detach(data)
#Attaching the data to R environment
attach(data)
#Fitting the model
model <- lm(log(utime)~.,data=data)
summary(model)
#Extracting specific parameter values
coef(summary(model))["duration","Pr(>|t|)"]
coef(summary(model))["duration","Estimate"]
#MLR Assumption validation
res = model$res
par(mfrow = c(4,4))
#Verifying Constant variance and Linearity
plot(utime, res, xlab = "utime", ylab = "Residuals", pch = 19, main = 'Residual vs utime')
abline(h = 0, col="red")
plot(height, res, xlab = "height", ylab = "Residuals", pch = 19, main = 'Residual vs height')
abline(h = 0,col="red")
plot(bitrate, res, xlab = "bitrate", ylab = "Residuals", pch = 19, main = 'Residual vs bitrate')
abline(h = 0,col="red")
plot(framerate, res, xlab = "framerate", ylab = "Residuals", pch = 19, main = 'Residual vs framerate')
abline(h = 0,col="red")
plot(i, res, xlab = "i", ylab = "Residuals", pch = 19, main = 'Residual vs i')
abline(h = 0,col="red")
plot(p, res, xlab = "p", ylab = "Residuals", pch = 19, main = 'Residual vs p')
abline(h = 0,col="red")
plot(b, res, xlab = "b", ylab = "Residuals", pch = 19, main = 'Residual vs b')
abline(h = 0,col="red")
plot(i_size, res, xlab = "i_size", ylab = "Residuals", pch = 19, main = 'Residual vs i_size')
abline(h = 0,col="red")
plot(p_size, res, xlab = "p_size", ylab = "Residuals", pch = 19, main = 'Residual vs p_size')
abline(h = 0,col="red")
plot(o_height, res, xlab = "o_height", ylab = "Residuals", pch = 19, main = 'Residual vs o_height')
abline(h = 0,col="red")
plot(o_bitrate, res, xlab = "o_bitrate", ylab = "Residuals", pch = 19, main = 'Residual vs o_bitrate')
abline(h = 0,col="red")
plot(o_framerate, res, xlab = "o_framerate", ylab = "Residuals", pch = 19, main = 'Residual vs o_framerate')
abline(h = 0,col="red")
plot(umem, res, xlab = "umem", ylab = "Residuals", pch = 19, main = 'Residual vs umem')
abline(h = 0,col="red")
#Verifying Normality of residuals
hist(res, xlab="Residuals", main= "Histogram of Residuals")
qqPlot(residuals(model),lwd=1, main = 'Normality of residuals')
#Identifying influencing datapoints
cookdist <- cooks.distance(model)
plot(cookdist, main = "Cook's Distance")
detach(data)
#Removing outlying points in Cook's dist and reverifying regression model
HighCDP <- which(cookdist >= 0.05)
#Printing high influential points
data[HighCDP,]
# Split the dataset (with outliers removed) by 90% (Train/Validation) and 10% (Final Test)
newdata<- (data[-HighCDP,])
# New model using log transformation
model_woHighCDP <- lm(log(utime)~.,data=newdata)
attach(newdata)
#MLR Assumption validation
res = model_woHighCDP$res
par(mfrow = c(4,4))
#Verifying Constant variance and Linearity
plot(utime, res, xlab = "utime", ylab = "Residuals", pch = 19, main = 'Residual vs utime')
abline(h = 0,col="red")
plot(height, res, xlab = "height", ylab = "Residuals", pch = 19, main = 'Residual vs height')
abline(h = 0,col="red")
plot(bitrate, res, xlab = "bitrate", ylab = "Residuals", pch = 19, main = 'Residual vs bitrate')
abline(h = 0,col="red")
plot(framerate, res, xlab = "framerate", ylab = "Residuals", pch = 19, main = 'Residual vs framerate')
abline(h = 0,col="red")
plot(i, res, xlab = "i", ylab = "Residuals", pch = 19, main = 'Residual vs i')
abline(h = 0,col="red")
plot(p, res, xlab = "p", ylab = "Residuals", pch = 19, main = 'Residual vs p')
abline(h = 0,col="red")
plot(b, res, xlab = "b", ylab = "Residuals", pch = 19, main = 'Residual vs b')
abline(h = 0,col="red")
plot(i_size, res, xlab = "i_size", ylab = "Residuals", pch = 19, main = 'Residual vs i_size')
abline(h = 0,col="red")
plot(p_size, res, xlab = "p_size", ylab = "Residuals", pch = 19, main = 'Residual vs p_size')
abline(h = 0,col="red")
plot(o_height, res, xlab = "o_height", ylab = "Residuals", pch = 19, main = 'Residual vs o_height')
abline(h = 0,col="red")
plot(o_bitrate, res, xlab = "o_bitrate", ylab = "Residuals", pch = 19, main = 'Residual vs o_bitrate')
abline(h = 0,col="red")
plot(o_framerate, res, xlab = "o_framerate", ylab = "Residuals", pch = 19, main = 'Residual vs o_framerate')
abline(h = 0,col="red")
plot(umem, res, xlab = "umem", ylab = "Residuals", pch = 19, main = 'Residual vs umem')
abline(h = 0,col="red")
#Verifying Normality of residuals
hist(res, xlab="Residuals", main= "Histogram of Residuals")
qqPlot(residuals(model_woHighCDP),lwd=1, main = 'Normality of residuals')
#Verifying Independence of residuals
plot(res, xlab = 'Sequence',ylab = 'Residuals', main = 'Residuals Independence')
#Identifying influencing datapoints
cookdist <- cooks.distance(model_woHighCDP)
plot(cookdist, main = "Cook's Distance")
